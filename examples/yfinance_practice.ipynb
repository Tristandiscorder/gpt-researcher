{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yfinance_Usecase \n",
    "- developing python definition funneled into langchain tool thus to be selected  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Type\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.tools import BaseTool\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_stock_price(ticker):\n",
    "    \"\"\"Method to get current stock price\"\"\"\n",
    "\n",
    "    ticker_data = yf.Ticker(ticker)\n",
    "    recent = ticker_data.history(period=\"6m\")\n",
    "    return {\"price\": recent.iloc[0][\"Close\"], \\\n",
    "            #\"currency\": ticker_data.info[\"currency\"]\n",
    "            }\n",
    "\n",
    "\n",
    "def get_stock_performance(ticker, days):\n",
    "    \"\"\"Method to get stock price change in percentage\"\"\"\n",
    "\n",
    "    past_date = datetime.today() - timedelta(days=days)\n",
    "    ticker_data = yf.Ticker(ticker)\n",
    "    history = ticker_data.history(start=past_date)\n",
    "    old_price = history.iloc[0][\"Close\"]\n",
    "    current_price = history.iloc[-1][\"Close\"]\n",
    "    return {\"percent_change\": ((current_price - old_price) / old_price) * 100}\n",
    "\n",
    "class CurrentStockPriceInput(BaseModel):\n",
    "    \"\"\"Inputs for get_current_stock_price\"\"\"\n",
    "\n",
    "    ticker: str = Field(description=\"Ticker symbol of the stock\")\n",
    "\n",
    "\n",
    "class CurrentStockPriceTool(BaseTool):\n",
    "    name = \"get_current_stock_price\"\n",
    "    description = \"\"\"\n",
    "        Useful when you want to get current stock price.\n",
    "        You should enter the stock ticker symbol recognized by the yahoo finance\n",
    "        \"\"\"\n",
    "    args_schema: Type[BaseModel] = CurrentStockPriceInput\n",
    "\n",
    "    def _run(self, ticker: str):\n",
    "        price_response = get_current_stock_price(ticker)\n",
    "        return price_response\n",
    "\n",
    "    def _arun(self, ticker: str):\n",
    "        raise NotImplementedError(\"get_current_stock_price does not support async\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockPerformanceInput(BaseModel):\n",
    "    \"\"\"Inputs for get_stock_performance\"\"\"\n",
    "\n",
    "    ticker: str = Field(description=\"Ticker symbol of the stock\")\n",
    "    days: int = Field(description=\"days duration since the given date to the latest trading day\")\n",
    "\n",
    "\n",
    "class StockPerformanceTool(BaseTool):\n",
    "    name = \"get_stock_performance\"\n",
    "    description = \"\"\"\n",
    "        Useful when you want to get the stock performance over the given period.\n",
    "        You should enter the stock ticker symbol recognized by the yahoo finance\n",
    "        \"\"\"\n",
    "    args_schema: Type[BaseModel] = StockPerformanceInput\n",
    "\n",
    "    def _run(self, ticker: str , days: int):\n",
    "        performance_response = get_stock_performance(ticker,days)\n",
    "        return performance_response\n",
    "\n",
    "    def _arun(self, ticker: str):\n",
    "        raise NotImplementedError(\"get_stock_performance does not support async\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentType, initialize_agent, Tool\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.tools.tavily_search import TavilySearchResults\n",
    "from langchain.utilities.tavily_search import TavilySearchAPIWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/ohsam/Documents/Github/gpt-researcher/examples/yfinance_practice.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/ohsam/Documents/Github/gpt-researcher/examples/yfinance_practice.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m tools \u001b[39m=\u001b[39m [CurrentStockPriceTool(),StockPerformanceTool(),tavily_tool\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/ohsam/Documents/Github/gpt-researcher/examples/yfinance_practice.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m ]\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/ohsam/Documents/Github/gpt-researcher/examples/yfinance_practice.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m agent \u001b[39m=\u001b[39m initialize_agent(tools, llm, agent\u001b[39m=\u001b[39mAgentType\u001b[39m.\u001b[39mSTRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/ohsam/Documents/Github/gpt-researcher/examples/yfinance_practice.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m agent\u001b[39m.\u001b[39mrun(\u001b[39m\"\u001b[39m\u001b[39mget me 3 data [the latest trading day stock price of Samsung Electronics(KRX), the date in which \u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/ohsam/Documents/Github/gpt-researcher/examples/yfinance_practice.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m          Korea banned short-selling, its performance since the ban].\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/ohsam/Documents/Github/gpt-researcher/examples/yfinance_practice.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mDo not assume any random dates if the article searched doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt say about it, you can tell me\u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/ohsam/Documents/Github/gpt-researcher/examples/yfinance_practice.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m            that article\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms release date and ultimately the performance since then\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/langchain/chains/base.py:505\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    504\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`run` supports only one positional argument.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 505\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(args[\u001b[39m0\u001b[39m], callbacks\u001b[39m=\u001b[39mcallbacks, tags\u001b[39m=\u001b[39mtags, metadata\u001b[39m=\u001b[39mmetadata)[\n\u001b[1;32m    506\u001b[0m         _output_key\n\u001b[1;32m    507\u001b[0m     ]\n\u001b[1;32m    509\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m    510\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(kwargs, callbacks\u001b[39m=\u001b[39mcallbacks, tags\u001b[39m=\u001b[39mtags, metadata\u001b[39m=\u001b[39mmetadata)[\n\u001b[1;32m    511\u001b[0m         _output_key\n\u001b[1;32m    512\u001b[0m     ]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/langchain/chains/base.py:310\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    309\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 310\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    311\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    312\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    313\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    314\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/langchain/chains/base.py:304\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    297\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    298\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    299\u001b[0m     inputs,\n\u001b[1;32m    300\u001b[0m     name\u001b[39m=\u001b[39mrun_name,\n\u001b[1;32m    301\u001b[0m )\n\u001b[1;32m    302\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    303\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 304\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs, run_manager\u001b[39m=\u001b[39mrun_manager)\n\u001b[1;32m    305\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    306\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    307\u001b[0m     )\n\u001b[1;32m    308\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    309\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/langchain/agents/agent.py:1167\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m   1165\u001b[0m \u001b[39m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[1;32m   1166\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[0;32m-> 1167\u001b[0m     next_step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_take_next_step(\n\u001b[1;32m   1168\u001b[0m         name_to_tool_map,\n\u001b[1;32m   1169\u001b[0m         color_mapping,\n\u001b[1;32m   1170\u001b[0m         inputs,\n\u001b[1;32m   1171\u001b[0m         intermediate_steps,\n\u001b[1;32m   1172\u001b[0m         run_manager\u001b[39m=\u001b[39mrun_manager,\n\u001b[1;32m   1173\u001b[0m     )\n\u001b[1;32m   1174\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[1;32m   1175\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return(\n\u001b[1;32m   1176\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[39m=\u001b[39mrun_manager\n\u001b[1;32m   1177\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/langchain/agents/agent.py:954\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m    951\u001b[0m     intermediate_steps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_intermediate_steps(intermediate_steps)\n\u001b[1;32m    953\u001b[0m     \u001b[39m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[0;32m--> 954\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent\u001b[39m.\u001b[39mplan(\n\u001b[1;32m    955\u001b[0m         intermediate_steps,\n\u001b[1;32m    956\u001b[0m         callbacks\u001b[39m=\u001b[39mrun_manager\u001b[39m.\u001b[39mget_child() \u001b[39mif\u001b[39;00m run_manager \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    957\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs,\n\u001b[1;32m    958\u001b[0m     )\n\u001b[1;32m    959\u001b[0m \u001b[39mexcept\u001b[39;00m OutputParserException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    960\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_parsing_errors, \u001b[39mbool\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/langchain/agents/agent.py:567\u001b[0m, in \u001b[0;36mAgent.plan\u001b[0;34m(self, intermediate_steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Given input, decided what to do.\u001b[39;00m\n\u001b[1;32m    556\u001b[0m \n\u001b[1;32m    557\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[39m    Action specifying what tool to use.\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    566\u001b[0m full_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_full_inputs(intermediate_steps, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 567\u001b[0m full_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm_chain\u001b[39m.\u001b[39mpredict(callbacks\u001b[39m=\u001b[39mcallbacks, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfull_inputs)\n\u001b[1;32m    568\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_parser\u001b[39m.\u001b[39mparse(full_output)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/langchain/chains/llm.py:298\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[0;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, callbacks: Callbacks \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m    284\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \n\u001b[1;32m    286\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[39m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 298\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(kwargs, callbacks\u001b[39m=\u001b[39mcallbacks)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/langchain/chains/base.py:310\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    309\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 310\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    311\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    312\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    313\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    314\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/langchain/chains/base.py:304\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    297\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    298\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    299\u001b[0m     inputs,\n\u001b[1;32m    300\u001b[0m     name\u001b[39m=\u001b[39mrun_name,\n\u001b[1;32m    301\u001b[0m )\n\u001b[1;32m    302\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    303\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 304\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs, run_manager\u001b[39m=\u001b[39mrun_manager)\n\u001b[1;32m    305\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    306\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    307\u001b[0m     )\n\u001b[1;32m    308\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    309\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/langchain/chains/llm.py:108\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\n\u001b[1;32m    104\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    105\u001b[0m     inputs: Dict[\u001b[39mstr\u001b[39m, Any],\n\u001b[1;32m    106\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    107\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m--> 108\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate([inputs], run_manager\u001b[39m=\u001b[39mrun_manager)\n\u001b[1;32m    109\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_outputs(response)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/langchain/chains/llm.py:120\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    118\u001b[0m callbacks \u001b[39m=\u001b[39m run_manager\u001b[39m.\u001b[39mget_child() \u001b[39mif\u001b[39;00m run_manager \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm, BaseLanguageModel):\n\u001b[0;32m--> 120\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm\u001b[39m.\u001b[39mgenerate_prompt(\n\u001b[1;32m    121\u001b[0m         prompts,\n\u001b[1;32m    122\u001b[0m         stop,\n\u001b[1;32m    123\u001b[0m         callbacks\u001b[39m=\u001b[39mcallbacks,\n\u001b[1;32m    124\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm_kwargs,\n\u001b[1;32m    125\u001b[0m     )\n\u001b[1;32m    126\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    127\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm\u001b[39m.\u001b[39mbind(stop\u001b[39m=\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm_kwargs)\u001b[39m.\u001b[39mbatch(\n\u001b[1;32m    128\u001b[0m         cast(List, prompts), {\u001b[39m\"\u001b[39m\u001b[39mcallbacks\u001b[39m\u001b[39m\"\u001b[39m: callbacks}\n\u001b[1;32m    129\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/langchain/chat_models/base.py:459\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_prompt\u001b[39m(\n\u001b[1;32m    452\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    453\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    457\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    458\u001b[0m     prompt_messages \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mto_messages() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m prompts]\n\u001b[0;32m--> 459\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate(prompt_messages, stop\u001b[39m=\u001b[39mstop, callbacks\u001b[39m=\u001b[39mcallbacks, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/langchain/chat_models/base.py:349\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n\u001b[1;32m    348\u001b[0m             run_managers[i]\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 349\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    350\u001b[0m flattened_outputs \u001b[39m=\u001b[39m [\n\u001b[1;32m    351\u001b[0m     LLMResult(generations\u001b[39m=\u001b[39m[res\u001b[39m.\u001b[39mgenerations], llm_output\u001b[39m=\u001b[39mres\u001b[39m.\u001b[39mllm_output)\n\u001b[1;32m    352\u001b[0m     \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results\n\u001b[1;32m    353\u001b[0m ]\n\u001b[1;32m    354\u001b[0m llm_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_combine_llm_outputs([res\u001b[39m.\u001b[39mllm_output \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/langchain/chat_models/base.py:339\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[39mfor\u001b[39;00m i, m \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(messages):\n\u001b[1;32m    337\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m         results\u001b[39m.\u001b[39mappend(\n\u001b[0;32m--> 339\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate_with_cache(\n\u001b[1;32m    340\u001b[0m                 m,\n\u001b[1;32m    341\u001b[0m                 stop\u001b[39m=\u001b[39mstop,\n\u001b[1;32m    342\u001b[0m                 run_manager\u001b[39m=\u001b[39mrun_managers[i] \u001b[39mif\u001b[39;00m run_managers \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    343\u001b[0m                 \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    344\u001b[0m             )\n\u001b[1;32m    345\u001b[0m         )\n\u001b[1;32m    346\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    347\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/langchain/chat_models/base.py:492\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    489\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    490\u001b[0m     )\n\u001b[1;32m    491\u001b[0m \u001b[39mif\u001b[39;00m new_arg_supported:\n\u001b[0;32m--> 492\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(\n\u001b[1;32m    493\u001b[0m         messages, stop\u001b[39m=\u001b[39mstop, run_manager\u001b[39m=\u001b[39mrun_manager, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    494\u001b[0m     )\n\u001b[1;32m    495\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    496\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(messages, stop\u001b[39m=\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/langchain/chat_models/openai.py:417\u001b[0m, in \u001b[0;36mChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m    415\u001b[0m message_dicts, params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[1;32m    416\u001b[0m params \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs}\n\u001b[0;32m--> 417\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompletion_with_retry(\n\u001b[1;32m    418\u001b[0m     messages\u001b[39m=\u001b[39mmessage_dicts, run_manager\u001b[39m=\u001b[39mrun_manager, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    419\u001b[0m )\n\u001b[1;32m    420\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/langchain/chat_models/openai.py:339\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry\u001b[0;34m(self, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Use tenacity to retry the completion call.\"\"\"\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[39mif\u001b[39;00m is_openai_v1():\n\u001b[0;32m--> 339\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    341\u001b[0m retry_decorator \u001b[39m=\u001b[39m _create_retry_decorator(\u001b[39mself\u001b[39m, run_manager\u001b[39m=\u001b[39mrun_manager)\n\u001b[1;32m    343\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[1;32m    344\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/openai/_utils/_utils.py:301\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    299\u001b[0m             msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMissing required argument: \u001b[39m\u001b[39m{\u001b[39;00mquote(missing[\u001b[39m0\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    300\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 301\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/openai/resources/chat/completions.py:598\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[39m@required_args\u001b[39m([\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], [\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    552\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    553\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    596\u001b[0m     timeout: \u001b[39mfloat\u001b[39m \u001b[39m|\u001b[39m httpx\u001b[39m.\u001b[39mTimeout \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m|\u001b[39m NotGiven \u001b[39m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    597\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ChatCompletion \u001b[39m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 598\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_post(\n\u001b[1;32m    599\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m/chat/completions\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    600\u001b[0m         body\u001b[39m=\u001b[39mmaybe_transform(\n\u001b[1;32m    601\u001b[0m             {\n\u001b[1;32m    602\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m: messages,\n\u001b[1;32m    603\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m: model,\n\u001b[1;32m    604\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mfrequency_penalty\u001b[39m\u001b[39m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m    605\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mfunction_call\u001b[39m\u001b[39m\"\u001b[39m: function_call,\n\u001b[1;32m    606\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mfunctions\u001b[39m\u001b[39m\"\u001b[39m: functions,\n\u001b[1;32m    607\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mlogit_bias\u001b[39m\u001b[39m\"\u001b[39m: logit_bias,\n\u001b[1;32m    608\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mmax_tokens\u001b[39m\u001b[39m\"\u001b[39m: max_tokens,\n\u001b[1;32m    609\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mn\u001b[39m\u001b[39m\"\u001b[39m: n,\n\u001b[1;32m    610\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mpresence_penalty\u001b[39m\u001b[39m\"\u001b[39m: presence_penalty,\n\u001b[1;32m    611\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mresponse_format\u001b[39m\u001b[39m\"\u001b[39m: response_format,\n\u001b[1;32m    612\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mseed\u001b[39m\u001b[39m\"\u001b[39m: seed,\n\u001b[1;32m    613\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mstop\u001b[39m\u001b[39m\"\u001b[39m: stop,\n\u001b[1;32m    614\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m: stream,\n\u001b[1;32m    615\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mtemperature\u001b[39m\u001b[39m\"\u001b[39m: temperature,\n\u001b[1;32m    616\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mtool_choice\u001b[39m\u001b[39m\"\u001b[39m: tool_choice,\n\u001b[1;32m    617\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mtools\u001b[39m\u001b[39m\"\u001b[39m: tools,\n\u001b[1;32m    618\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mtop_p\u001b[39m\u001b[39m\"\u001b[39m: top_p,\n\u001b[1;32m    619\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m: user,\n\u001b[1;32m    620\u001b[0m             },\n\u001b[1;32m    621\u001b[0m             completion_create_params\u001b[39m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m    622\u001b[0m         ),\n\u001b[1;32m    623\u001b[0m         options\u001b[39m=\u001b[39mmake_request_options(\n\u001b[1;32m    624\u001b[0m             extra_headers\u001b[39m=\u001b[39mextra_headers, extra_query\u001b[39m=\u001b[39mextra_query, extra_body\u001b[39m=\u001b[39mextra_body, timeout\u001b[39m=\u001b[39mtimeout\n\u001b[1;32m    625\u001b[0m         ),\n\u001b[1;32m    626\u001b[0m         cast_to\u001b[39m=\u001b[39mChatCompletion,\n\u001b[1;32m    627\u001b[0m         stream\u001b[39m=\u001b[39mstream \u001b[39mor\u001b[39;00m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    628\u001b[0m         stream_cls\u001b[39m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[1;32m    629\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/openai/_base_client.py:1063\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(\n\u001b[1;32m   1050\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1051\u001b[0m     path: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1058\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1059\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[1;32m   1060\u001b[0m     opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[1;32m   1061\u001b[0m         method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39mto_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[1;32m   1062\u001b[0m     )\n\u001b[0;32m-> 1063\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(ResponseT, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest(cast_to, opts, stream\u001b[39m=\u001b[39mstream, stream_cls\u001b[39m=\u001b[39mstream_cls))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/openai/_base_client.py:842\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    833\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    834\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    835\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    840\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    841\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[0;32m--> 842\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_request(\n\u001b[1;32m    843\u001b[0m         cast_to\u001b[39m=\u001b[39mcast_to,\n\u001b[1;32m    844\u001b[0m         options\u001b[39m=\u001b[39moptions,\n\u001b[1;32m    845\u001b[0m         stream\u001b[39m=\u001b[39mstream,\n\u001b[1;32m    846\u001b[0m         stream_cls\u001b[39m=\u001b[39mstream_cls,\n\u001b[1;32m    847\u001b[0m         remaining_retries\u001b[39m=\u001b[39mremaining_retries,\n\u001b[1;32m    848\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/openai/_base_client.py:873\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[39mexcept\u001b[39;00m httpx\u001b[39m.\u001b[39mHTTPStatusError \u001b[39mas\u001b[39;00m err:  \u001b[39m# thrown on 4xx and 5xx status code\u001b[39;00m\n\u001b[1;32m    872\u001b[0m     \u001b[39mif\u001b[39;00m retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_retry(err\u001b[39m.\u001b[39mresponse):\n\u001b[0;32m--> 873\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retry_request(\n\u001b[1;32m    874\u001b[0m             options,\n\u001b[1;32m    875\u001b[0m             cast_to,\n\u001b[1;32m    876\u001b[0m             retries,\n\u001b[1;32m    877\u001b[0m             err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mheaders,\n\u001b[1;32m    878\u001b[0m             stream\u001b[39m=\u001b[39mstream,\n\u001b[1;32m    879\u001b[0m             stream_cls\u001b[39m=\u001b[39mstream_cls,\n\u001b[1;32m    880\u001b[0m         )\n\u001b[1;32m    882\u001b[0m     \u001b[39m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    883\u001b[0m     \u001b[39m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    884\u001b[0m     err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mread()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/openai/_base_client.py:933\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[39m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m    930\u001b[0m \u001b[39m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m    931\u001b[0m time\u001b[39m.\u001b[39msleep(timeout)\n\u001b[0;32m--> 933\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_request(\n\u001b[1;32m    934\u001b[0m     options\u001b[39m=\u001b[39moptions,\n\u001b[1;32m    935\u001b[0m     cast_to\u001b[39m=\u001b[39mcast_to,\n\u001b[1;32m    936\u001b[0m     remaining_retries\u001b[39m=\u001b[39mremaining,\n\u001b[1;32m    937\u001b[0m     stream\u001b[39m=\u001b[39mstream,\n\u001b[1;32m    938\u001b[0m     stream_cls\u001b[39m=\u001b[39mstream_cls,\n\u001b[1;32m    939\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/openai/_base_client.py:873\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[39mexcept\u001b[39;00m httpx\u001b[39m.\u001b[39mHTTPStatusError \u001b[39mas\u001b[39;00m err:  \u001b[39m# thrown on 4xx and 5xx status code\u001b[39;00m\n\u001b[1;32m    872\u001b[0m     \u001b[39mif\u001b[39;00m retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_retry(err\u001b[39m.\u001b[39mresponse):\n\u001b[0;32m--> 873\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retry_request(\n\u001b[1;32m    874\u001b[0m             options,\n\u001b[1;32m    875\u001b[0m             cast_to,\n\u001b[1;32m    876\u001b[0m             retries,\n\u001b[1;32m    877\u001b[0m             err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mheaders,\n\u001b[1;32m    878\u001b[0m             stream\u001b[39m=\u001b[39mstream,\n\u001b[1;32m    879\u001b[0m             stream_cls\u001b[39m=\u001b[39mstream_cls,\n\u001b[1;32m    880\u001b[0m         )\n\u001b[1;32m    882\u001b[0m     \u001b[39m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    883\u001b[0m     \u001b[39m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    884\u001b[0m     err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mread()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/openai/_base_client.py:933\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[39m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m    930\u001b[0m \u001b[39m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m    931\u001b[0m time\u001b[39m.\u001b[39msleep(timeout)\n\u001b[0;32m--> 933\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_request(\n\u001b[1;32m    934\u001b[0m     options\u001b[39m=\u001b[39moptions,\n\u001b[1;32m    935\u001b[0m     cast_to\u001b[39m=\u001b[39mcast_to,\n\u001b[1;32m    936\u001b[0m     remaining_retries\u001b[39m=\u001b[39mremaining,\n\u001b[1;32m    937\u001b[0m     stream\u001b[39m=\u001b[39mstream,\n\u001b[1;32m    938\u001b[0m     stream_cls\u001b[39m=\u001b[39mstream_cls,\n\u001b[1;32m    939\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/openai/_base_client.py:885\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[39m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    883\u001b[0m     \u001b[39m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    884\u001b[0m     err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mread()\n\u001b[0;32m--> 885\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_status_error_from_response(err\u001b[39m.\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    886\u001b[0m \u001b[39mexcept\u001b[39;00m httpx\u001b[39m.\u001b[39mTimeoutException \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    887\u001b[0m     \u001b[39mif\u001b[39;00m retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0, openai_api_key=\"\")\n",
    "\n",
    "search = TavilySearchAPIWrapper()\n",
    "tavily_tool = TavilySearchResults(api_wrapper=search)\n",
    "tools = [CurrentStockPriceTool(),StockPerformanceTool(),tavily_tool\n",
    "]\n",
    "\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "agent.run(\"get me 3 data [the latest trading day stock price of Samsung Electronics(KRX), the date in which \\\n",
    "          Korea banned short-selling, its performance since the ban].\\n\"\n",
    "          \"Do not assume any random dates if the article searched doesn't say about it, you can tell me\\\n",
    "            that article's release date and ultimately the performance since then\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
